"""
Main application entry point for APIKEY-king.
"""

import argparse
import sys
import time
from datetime import datetime
from typing import Dict, Any, List

from .core import ScanMode, APIKeyScanner
from .services import ConfigService, GitHubService, FileService
from .extractors import GeminiExtractor, ModelScopeExtractor, OpenRouterExtractor
from .validators import GeminiValidator, OpenRouterValidator, ModelScopeValidator
from .models import Checkpoint, ScanResult, BatchScanResult
from .utils import logger


class Application:
    """Main application class."""
    
    def __init__(self, scan_mode: ScanMode = ScanMode.COMPATIBLE):
        self.config_service = ConfigService()
        self.config = None
        self.github_service = None
        self.file_service = None
        self.scanner = None
        self.checkpoint = None
        self.scan_mode = scan_mode
        
        # Statistics
        self.skip_stats = {
            "time_filter": 0,
            "sha_duplicate": 0,
            "age_filter": 0,
            "doc_filter": 0
        }

        # Running statistics for real-time progress
        self.running_stats = {"extracted": 0, "validated": 0, "valid": 0}
        self.last_progress_time = 0
    
    def initialize(self) -> bool:
        """Initialize application components."""
        try:
            # Load configuration
            self.config = self.config_service.load_config()
            
            # Apply scan mode overrides
            self._apply_scan_mode_config()
            
            logger.init_success("é…ç½® Configuration")
            
            # Initialize services
            self.github_service = GitHubService(self.config)
            self.file_service = FileService(self.config.data_path)
            logger.init_success("æœåŠ¡ Services")
            
            # Create extractors based on configuration
            extractors = self._create_extractors()
            validators = self._create_validators()
            
            # Create scanner
            self.scanner = APIKeyScanner(extractors, validators)
            logger.scan_mode_summary(extractors, validators)
            
            # Load checkpoint
            self.checkpoint = self.file_service.load_checkpoint()
            logger.init_success("æ£€æŸ¥ç‚¹ Checkpoint")
            
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥ Failed to initialize application: {e}")
            return False
    
    def _apply_scan_mode_config(self) -> None:
        """Apply scan mode specific configuration overrides."""
        if self.scan_mode == ScanMode.OPENROUTER_ONLY:
            # Enable only OpenRouter extractor
            for name, extractor_config in self.config.extractors.items():
                if name == 'openrouter':
                    extractor_config.enabled = True
                    extractor_config.extract_only = False  # Enable validation for OpenRouter
                else:
                    extractor_config.enabled = False
            
            # Enable only OpenRouter validator
            for name, validator_config in self.config.validators.items():
                if name == 'openrouter':
                    validator_config.enabled = True
                else:
                    validator_config.enabled = False
            
            # Use OpenRouter queries
            self.config.scan.queries_file = "config/queries/openrouter.txt"
            logger.mode_activated("OpenRouter", True)
            
        elif self.scan_mode == ScanMode.MODELSCOPE_ONLY:
            # Enable only ModelScope extractor
            for name, extractor_config in self.config.extractors.items():
                if name == 'modelscope':
                    extractor_config.enabled = True
                    extractor_config.extract_only = False  # Enable validation for ModelScope
                else:
                    extractor_config.enabled = False
            
            # Enable only ModelScope validator
            for name, validator_config in self.config.validators.items():
                if name == 'modelscope':
                    validator_config.enabled = True
                else:
                    validator_config.enabled = False
            
            # Use ModelScope queries
            self.config.scan.queries_file = "config/queries/modelscope.txt"
            logger.mode_activated("ModelScope", True)
            
        elif self.scan_mode == ScanMode.GEMINI_ONLY:
            # Enable only Gemini extractor
            for name, extractor_config in self.config.extractors.items():
                if name == 'gemini':
                    extractor_config.enabled = True
                    extractor_config.extract_only = False  # Enable validation
                else:
                    extractor_config.enabled = False
            
            # Enable only Gemini validator
            for name, validator_config in self.config.validators.items():
                if name == 'gemini':
                    validator_config.enabled = True
                else:
                    validator_config.enabled = False
            
            # Use Gemini queries
            self.config.scan.queries_file = "config/queries/gemini.txt"
            logger.mode_activated("Gemini", True)

        elif self.scan_mode == ScanMode.SILICONFLOW_ONLY:
            # Enable only SiliconFlow extractor
            for name, extractor_config in self.config.extractors.items():
                if name == 'siliconflow':
                    extractor_config.enabled = True
                    extractor_config.extract_only = False  # Enable validation
                else:
                    extractor_config.enabled = False

            # Enable only SiliconFlow validator
            for name, validator_config in self.config.validators.items():
                if name == 'siliconflow':
                    validator_config.enabled = True
                else:
                    validator_config.enabled = False

            # Use SiliconFlow queries
            self.config.scan.queries_file = "config/queries/siliconflow.txt"
            logger.mode_activated("SiliconFlow", True)

        elif self.scan_mode == ScanMode.COMPATIBLE:
            # Enable all extractors with validation where supported
            for name, extractor_config in self.config.extractors.items():
                if name in ['gemini', 'openrouter', 'modelscope', 'siliconflow']:
                    extractor_config.extract_only = False  # Enable validation for all
            
            # Enable all validators
            for name, validator_config in self.config.validators.items():
                validator_config.enabled = True
                
            logger.mode_activated("å…¼å®¹ Compatible", True)
    
    def _create_extractors(self) -> List:
        """Create extractor instances based on configuration."""
        extractors = []
        
        enabled_extractors = self.config.get_enabled_extractors()
        
        for name, config in enabled_extractors.items():
            try:
                if name == 'gemini':
                    extractor = GeminiExtractor(config)
                elif name == 'modelscope':
                    extractor = ModelScopeExtractor(config)
                elif name == 'openrouter':
                    extractor = OpenRouterExtractor(config)
                elif name == 'siliconflow':
                    from .extractors.siliconflow import SiliconFlowExtractor
                    extractor = SiliconFlowExtractor(config)
                else:
                    logger.warning(f"Unknown extractor type: {name}")
                    continue
                
                extractors.append(extractor)
                logger.init_success(f"{name} æå–å™¨ extractor")
                
            except Exception as e:
                logger.error(f"Failed to create {name} extractor: {e}")
        
        return extractors
    
    def _create_validators(self) -> List:
        """Create validator instances based on configuration."""
        validators = []
        
        enabled_validators = self.config.get_enabled_validators()
        
        for name, config in enabled_validators.items():
            try:
                if name == 'gemini':
                    validator = GeminiValidator(config)
                    validators.append(validator)
                    logger.init_success(f"{name} éªŒè¯å™¨ validator")
                elif name == 'openrouter':
                    validator = OpenRouterValidator(config)
                    validators.append(validator)
                    logger.init_success(f"{name} éªŒè¯å™¨ validator")
                elif name == 'modelscope':
                    validator = ModelScopeValidator(config)
                    validators.append(validator)
                    logger.init_success(f"{name} éªŒè¯å™¨ validator")
                elif name == 'siliconflow':
                    from .validators.siliconflow import SiliconFlowValidator
                    validator = SiliconFlowValidator(config)
                    validators.append(validator)
                    logger.init_success(f"{name} éªŒè¯å™¨ validator")
                else:
                    logger.warning(f"Unknown validator type: {name}")
            
            except Exception as e:
                logger.error(f"Failed to create {name} validator: {e}")
        
        return validators
    
    def run(self) -> None:
        """Run the main scanning loop."""
        if not self.initialize():
            logger.error("åˆå§‹åŒ–å¤±è´¥ Failed to initialize application")
            sys.exit(1)
        
        # Print startup information
        self._print_startup_info()
        
        # Load queries
        queries = self.file_service.load_queries(self.config.scan.queries_file)
        if not queries:
            logger.error("æœªåŠ è½½æŸ¥è¯¢ No queries loaded - check queries file")
            sys.exit(1)
        
        logger.info(f"ğŸ” å·²åŠ è½½ Loaded {len(queries)} ä¸ªæŸ¥è¯¢ queries")
        
        # Main scanning loop
        total_valid_keys = 0
        loop_count = 0
        
        try:
            while True:
                loop_count += 1
                logger.separator(f"ğŸ”„ ç¬¬ #{loop_count} è½® LOOP - {datetime.now().strftime('%H:%M:%S')}")
                
                batch_result = BatchScanResult()
                self._reset_skip_stats()
                
                for i, query in enumerate(queries, 1):
                    normalized_query = self._normalize_query(query)
                    
                    # Skip if already processed
                    if normalized_query in self.checkpoint.processed_queries:
                        logger.info(f"â­ï¸ è·³è¿‡å·²å¤„ç†æŸ¥è¯¢ Skipping processed query #{i}: {query[:50]}...")
                        continue
                    
                    logger.info(f"ğŸ” å¤„ç†æŸ¥è¯¢ Processing query #{i}/{len(queries)}: {query}")
                    
                    # Search GitHub
                    search_results = self.github_service.search_code(query)
                    items = search_results.get('items', [])
                    
                    if not items:
                        logger.info(f"ğŸ“­ æœªæ‰¾åˆ°æ¡ç›® No items found for query #{i}")
                        self.checkpoint.add_processed_query(normalized_query)
                        continue
                    
                    # Process items
                    for item_idx, item in enumerate(items, 1):
                        try:
                            if self._should_skip_item(item):
                                continue
                            
                            # Get file content
                            content = self.github_service.get_file_content(item)
                            if not content:
                                continue
                            
                            # Scan content
                            scan_results = self._scan_item(item, content)
                            if scan_results:
                                batch_result.add_result(scan_results)
                                total_valid_keys += len(scan_results.get_valid_keys())
                            
                            # Update checkpoint
                            self.checkpoint.add_scanned_sha(item.get("sha"))
                            
                            # Progress update every 20 items
                            if item_idx % 20 == 0:
                                logger.progress(f"Query {i} progress", item_idx, len(items))
                                self._save_checkpoint()
                        
                        except Exception as e:
                            logger.error(f"å¤„ç†æ¡ç›®é”™è¯¯ Error processing item {item_idx}: {e}")
                            batch_result.add_error({
                                'item_index': item_idx,
                                'error': str(e),
                                'item_url': item.get('html_url', 'unknown')
                            })
                    
                    # Mark query as processed
                    self.checkpoint.add_processed_query(normalized_query)
                    self._save_checkpoint()
                    
                    logger.info(f"âœ… æŸ¥è¯¢ #{i} å®Œæˆ Query complete - å‘ç° Found {len([r for r in batch_result.results if r.get_all_keys()])} ä¸ªæ–‡ä»¶åŒ…å«å¯†é’¥ files with keys")
                    self._print_skip_stats()
                
                # Save batch results
                batch_result.finalize()
                self.file_service.save_batch_result(batch_result)
                
                logger.separator(f"ğŸ ç¬¬ #{loop_count} è½®å®Œæˆ LOOP COMPLETE")
                logger.success(f"å·²å¤„ç† Processed {batch_result.total_files_processed} ä¸ªæ–‡ä»¶ files")
                logger.success(f"å‘ç° Found {batch_result.total_keys_found} ä¸ªå¯†é’¥ total keys")  
                logger.success(f"éªŒè¯ Validated {batch_result.total_valid_keys} ä¸ªæœ‰æ•ˆå¯†é’¥ valid keys")
                logger.info(f"ç´¯è®¡æœ‰æ•ˆå¯†é’¥ Total valid keys so far: {total_valid_keys}")
                
                # Sleep between loops
                logger.info("ğŸ’¤ ä¼‘çœ  10 ç§’ç­‰å¾…ä¸‹ä¸€è½® Sleeping 10 seconds before next loop...")
                time.sleep(10)
        
        except KeyboardInterrupt:
            logger.info("â›” ç”¨æˆ·ä¸­æ–­ Interrupted by user")

            # Save current batch results if any
            if 'batch_result' in locals() and batch_result.results:
                batch_result.finalize()
                self.file_service.save_batch_result(batch_result)
                logger.info(f"ğŸ’¾ å·²ä¿å­˜å½“å‰æ‰¹æ¬¡ç»“æœ Saved current batch results")

            # Display final file locations
            mode_name = self.scan_mode.value if hasattr(self.scan_mode, 'value') else str(self.scan_mode)
            output_files = self.file_service.get_output_file_paths(mode_name)
            logger.separator("ğŸ“ å¯†é’¥æ–‡ä»¶ä½ç½® Key File Locations")

            from pathlib import Path
            for file_type, file_path in output_files.items():
                if Path(file_path).exists():
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            key_count = sum(1 for line in f if line.strip())
                        logger.info(f"ğŸ’¾ {file_type}: {file_path} ({key_count} keys)")
                    except Exception:
                        logger.info(f"ğŸ’¾ {file_type}: {file_path} (file exists)")
                else:
                    logger.info(f"ğŸ“„ {file_type}: {file_path} (not created)")

            self._save_checkpoint()
            logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡ Final stats - æ€»å…±å‘ç°æœ‰æ•ˆå¯†é’¥ Total valid keys found: {total_valid_keys}")
    
    def _scan_item(self, item: Dict[str, Any], content: str) -> ScanResult:
        """Scan a single item with enhanced logging and immediate file saving."""
        context = {
            'file_path': item.get('path', ''),
            'repository': item.get('repository', {}),
            'proxy_config': None  # Could add proxy config here
        }

        repo_name = item.get('repository', {}).get('full_name', '')
        file_path = item.get('path', '')
        file_url = item.get('html_url', '')

        # Use scanner to process content
        scan_results = self.scanner.scan_content(content, context)

        if not scan_results['summary']['total_keys_found']:
            return None

        # Log extraction results immediately
        for extractor_name, extraction_result in scan_results['extracted_keys'].items():
            if extraction_result.keys:
                logger.key_extracted(extractor_name, len(extraction_result.keys), repo_name, file_path)

        # Process validation results with immediate logging and saving
        total_extracted = scan_results['summary']['total_keys_found']
        total_valid = 0

        for key, validation_result in scan_results['validation_results'].items():
            # Determine key type from key format
            key_type = self._determine_key_type(key)
            key_prefix = key[:12] + "..." if len(key) > 12 else key

            # Log validation start
            logger.key_validating(key_prefix, key_type)

            # Log validation result and save immediately
            if validation_result.is_valid:
                logger.key_validation_success(key_prefix, key_type)
                saved_file = self.file_service.save_key_immediately(
                    key, key_type, repo_name, file_path, file_url,
                    {'is_valid': True, 'status': 'valid'}
                )
                if saved_file:
                    logger.key_saved_immediately(key_prefix, saved_file)
                total_valid += 1
            else:
                reason = validation_result.error_message or validation_result.status or "Unknown"
                logger.key_validation_failed(key_prefix, key_type, reason)

                # Save rate-limited keys immediately
                if validation_result.status == 'rate_limited':
                    saved_file = self.file_service.save_key_immediately(
                        key, key_type, repo_name, file_path, file_url,
                        {'is_valid': False, 'status': 'rate_limited'}
                    )
                    if saved_file:
                        logger.key_saved_immediately(key_prefix, saved_file)

        # Update running totals and log progress
        self.running_stats['extracted'] += total_extracted
        self.running_stats['validated'] += len(scan_results['validation_results'])
        self.running_stats['valid'] += total_valid

        # Log progress summary every 10 keys or every 2 minutes
        import time
        if (self.running_stats['extracted'] % 10 == 0 or
            time.time() - self.last_progress_time > 120):
            logger.progress_summary(
                self.running_stats['extracted'],
                self.running_stats['validated'],
                self.running_stats['valid']
            )
            self.last_progress_time = time.time()

        # Create ScanResult object (for compatibility)
        result = ScanResult(
            file_url=file_url,
            repository_name=repo_name,
            file_path=file_path,
            extracted_keys={},
            validation_results={},
            scan_metadata=scan_results['summary']
        )

        # Process extraction results
        for extractor_name, extraction_result in scan_results['extracted_keys'].items():
            result.extracted_keys[extractor_name] = extraction_result.keys

        # Process validation results
        for key, validation_result in scan_results['validation_results'].items():
            result.validation_results[key] = {
                'is_valid': validation_result.is_valid,
                'status': validation_result.status,
                'error_message': validation_result.error_message,
                'metadata': validation_result.metadata
            }

        return result

    def _determine_key_type(self, key: str) -> str:
        """Determine key type from key format."""
        if key.startswith('AIzaSy'):
            return 'gemini'
        elif key.startswith('sk-or-v1-'):
            return 'openrouter'
        elif key.startswith('ms-'):
            return 'modelscope'
        elif key.startswith('sk-'):
            return 'siliconflow'
        else:
            return 'unknown'

    def _should_skip_item(self, item: Dict[str, Any]) -> bool:
        """Check if item should be skipped."""
        # Check if SHA already processed
        if item.get("sha") in self.checkpoint.scanned_shas:
            self.skip_stats["sha_duplicate"] += 1
            return True
        
        # Check file path blacklist
        file_path = item.get("path", "").lower()
        if any(blocked in file_path for blocked in self.config.scan.file_path_blacklist):
            self.skip_stats["doc_filter"] += 1
            return True
        
        return False
    
    def _normalize_query(self, query: str) -> str:
        """Normalize query string for consistency."""
        return " ".join(query.split())
    
    def _save_checkpoint(self) -> None:
        """Save current checkpoint."""
        self.checkpoint.update_scan_time()
        self.file_service.save_checkpoint(self.checkpoint)
    
    def _reset_skip_stats(self) -> None:
        """Reset skip statistics."""
        self.skip_stats = {"time_filter": 0, "sha_duplicate": 0, "age_filter": 0, "doc_filter": 0}
    
    def _print_skip_stats(self) -> None:
        """Print skip statistics."""
        total_skipped = sum(self.skip_stats.values())
        if total_skipped > 0:
            logger.info(f"ğŸ“Š å·²è·³è¿‡ Skipped {total_skipped} ä¸ªæ¡ç›® items - " + 
                       f"é‡å¤ Duplicate: {self.skip_stats['sha_duplicate']}, " +
                       f"æ–‡æ¡£ Docs: {self.skip_stats['doc_filter']}")
    
    def _print_startup_info(self) -> None:
        """Print enhanced startup information."""
        logger.startup_banner()
        logger.github_tokens(len(self.config.github.tokens))
        logger.info(f"ğŸ“… æ—¥æœŸè¿‡æ»¤ Date filter: {self.config.scan.date_range_days} å¤© days")

        if self.config.get_proxy_configs():
            logger.info(f"ğŸŒ ä»£ç†é…ç½® Proxies: {len(self.config.get_proxy_configs())} ä¸ªå·²é…ç½® configured")

        # Display current scan mode
        mode_name = self.scan_mode.value if hasattr(self.scan_mode, 'value') else str(self.scan_mode)
        logger.mode_activated(mode_name, validation=True)

        # Display output file paths
        output_files = self.file_service.get_output_file_paths(mode_name)
        logger.output_files_info(mode_name, output_files)

        if self.checkpoint.last_scan_time:
            logger.info(f"ğŸ’¾ å¢é‡æ‰«ææ¨¡å¼ Incremental scan mode - ä¸Šæ¬¡æ‰«æ Last scan: {self.checkpoint.last_scan_time}")
            logger.info(f"ğŸ“ å·²æ‰«ææ–‡ä»¶ Already scanned: {len(self.checkpoint.scanned_shas)} ä¸ªæ–‡ä»¶ files")
        else:
            logger.info("ğŸ’¾ å®Œæ•´æ‰«ææ¨¡å¼ Full scan mode")

        logger.success("ç³»ç»Ÿå°±ç»ª System ready - å¼€å§‹æ‰«æ Starting scan")


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="APIKEY-king - API Key Discovery Tool")
    parser.add_argument(
        "--mode",
        choices=["modelscope-only", "openrouter-only", "gemini-only", "siliconflow-only", "compatible"],
        default="compatible",
        help="Scanning mode: modelscope-only, openrouter-only, gemini-only, siliconflow-only, or compatible (all types)"
    )
    parser.add_argument(
        "--config-preset",
        help="Load configuration preset from config/presets/ directory"
    )
    parser.add_argument(
        "--queries",
        help="Override queries file path"
    )
    return parser.parse_args()


def main():
    """Main entry point."""
    args = parse_args()
    
    # Determine scan mode
    scan_mode = ScanMode.COMPATIBLE
    if args.mode:
        scan_mode = ScanMode(args.mode)
    
    app = Application(scan_mode)
    
    # Apply CLI overrides
    if args.mode:
        logger.info(f"ğŸ§­ CLI Mode: {args.mode}")
    
    if args.config_preset:
        # Load preset configuration
        import os
        preset_file = f"config/presets/{args.config_preset}.env"
        if os.path.exists(preset_file):
            from dotenv import load_dotenv
            load_dotenv(preset_file, override=True)
            logger.info(f"ğŸ“‹ Loaded preset: {args.config_preset}")
        else:
            logger.error(f"âŒ Preset not found: {preset_file}")
            sys.exit(1)
    
    app.run()


if __name__ == "__main__":
    main()